---
title: "Zonal Statistics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Zonal Statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



With a weight grid, zonal metrics can be computed. The three primary approaches use slightly different processes:

  1. `exactextractr` leverages C libraries and an in memory raster and sf object. It works polygon-by-polygon to compute coverage's and the weight table is computed within the function.
  2. `intersectr` utilizes NetCDF filepath and calculates all polygons timestep-by-timestep using` data.table`. A weight grid must be supplied.
  3. `zonal` works from NetCDF or tif filepath and calculates all polygons and all time simultaneously using `data.table`. A weight grid must also be supplied.
  
The performance and comparison of these three approaches are shown below when the domain is large, and when (A) there a many thousands of polygons, and (B) when there a a few large polygon aggregation units.

### Option 1: Intersectr: 

The `intersectr` workflow for defining inputs for `execute_intersection` are wrapping into a prep function below:


```r
intersectr_prep = function(file, geom, ID, variable){
  nc_coord_vars <- nc_coord_var(file)
  nc_coord_vars <- filter(nc_coord_vars, variable == !!variable)
  
  nc       <- open.nc(file)
  X_coords <- var.get.nc(nc, nc_coord_vars$X, unpack = TRUE)
  Y_coords <- var.get.nc(nc, nc_coord_vars$Y, unpack = TRUE)
  
  nc_prj <- nc_gm_to_prj(nc_grid_mapping_atts(file))
    
  cell_geometry = create_cell_geometry(X_coords = X_coords,
                         Y_coords = Y_coords,
                         prj = nc_prj,
                         geom = geom, 
                         buffer_dist = 0.1, # Degrees
                         regularize = TRUE)
    
  data_source_cells <- st_sf(dplyr::select(cell_geometry, grid_ids))
  target_polygons   <- st_sf(dplyr::select(geom, !!ID))
  st_agr(data_source_cells) <- "constant"
  st_agr(target_polygons)   <- "constant"

  area_weights = calculate_area_intersection_weights(data_source_cells, target_polygons, allow_lonlat = TRUE)
  
  return(list(grid = cell_geometry, w = area_weights, x = nc_coord_vars$X, y = nc_coord_vars$Y, t = nc_coord_vars$T))
}
```

### Option 2: exactextract: 

The `exacextract` workflow for computing aggregate means for a raster stack are wrapped below:


```r
exactrextract_process = function(file, geom, ID){
  R.utils::withTimeout(
    exactextractr::exact_extract(raster::stack(file), 
                                 geom, 
                                 stack_apply = TRUE, 
                                 fun = "mean", 
                                 append_cols = ID,
                                 progress = FALSE),
  timeout = 180, onTimeout = "silent")
}
```

**Spoiler Alert**: This method can take an extremely long time when the polygon count is very high. As such, we are limiting the execution time to 180 seconds (three minutes). If a benchmark time indicates the process takes 180 seconds, it means the process was killed and not completed.

### Option 3: zonal: 

The `zonal` workflow for building a weight grid and executing the areal averages can be wrapped into a function below:


```r
zonal_full = function(file, geom, ID){
  w = weighting_grid(file, geom, ID, progress = FALSE)
  execute_zonal(file, w)
}
```

### Grid
The gridded data and aggregate units we are working with can be seen below:


```r
file = '/Users/mjohnson/Downloads/pet_1979.nc'
(s = raster::stack(file))
#> class      : RasterStack 
#> dimensions : 585, 1386, 810810, 365  (nrow, ncol, ncell, nlayers)
#> resolution : 0.04166667, 0.04166667  (x, y)
#> extent     : -124.7875, -67.0375, 25.04583, 49.42083  (xmin, xmax, ymin, ymax)
#> crs        : +proj=longlat +ellps=WGS84 +no_defs 
#> names      : X28854, X28855, X28856, X28857, X28858, X28859, X28860, X28861, X28862, X28863, X28864, X28865, X28866, X28867, X28868, ...
```

Looking at the grid we can see in consists of 8.1081 &times; 10<sup>5</sup> grid cells each with a 0.0416667 meter by 0.0416667 meter resolution. Additionally, there are 365 unique time slices in the NetCDF file.

# Example 1: HUC01

Our first example uses a hydrofabric developed for the Northeast USA.


```r
geom <- read_sf('/Users/mjohnson/github/hydrofabric/workflow/nhd_workflows/cache/ngen_01a-4.gpkg', "catchments") %>% 
  st_make_valid()
```

In total there are 19700 catchment features. Both zonal and intersectr are designed to precomute a weight grid. Therefore we time how long it takes to do this using each method:


```r
int_time_huc01 = system.time({
  intersectr_input = intersectr_prep(file, geom, ID = "comid", variable = 'potential_evapotranspiration')
})

zonal_time_huc01 = system.time({
  zonal_w = weighting_grid(file, geom, ID = "comid", progress = FALSE)
})
```

Here we benchmark the time it takes to do the following:
- run the `intersectr` workflow with precomputed weights
- run the `exactextractr` workflow
- run the `zonal` workflow with precomputed weights
- run the `zonal` workflow without precomputed weights



```r
huc01_bnch <- bench::mark(
  iterations = 1, check = FALSE, time_unit = 's',
  intersectr_stage_weights = execute_intersection(nc_file = file,
                               variable_name = 'potential_evapotranspiration',
                               intersection_weights = intersectr_input$w,
                               cell_geometry = intersectr_input$grid, 
                               x_var = intersectr_input$x,
                               y_var = intersectr_input$y,
                               t_var = intersectr_input$t, 
                               start_datetime = NULL, 
                               end_datetime = NULL),
  exactextractr            = exactrextract_process(file, geom, "comid"),
  zonal_stage_weights      = execute_zonal(file, zonal_w),
  zonal_full               = zonal_full(file, geom, "comid")
)
```


<table class="table table-condensed">
 <thead>
  <tr>
   <th style="text-align:right;"> expression </th>
   <th style="text-align:right;"> medianTime </th>
   <th style="text-align:right;"> memoryAllocated </th>
   <th style="text-align:right;"> medianTime_rel </th>
   <th style="text-align:right;"> memoryAllocated_rel </th>
   <th style="text-align:right;"> preTime </th>
   <th style="text-align:right;"> TotalTime </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> zonal_stage_weights </td>
   <td style="text-align:right;"> 31.22744 </td>
   <td style="text-align:right;"> 699.133057MB </td>
   <td style="text-align:right;"> 1.000000 </td>
   <td style="text-align:right;"> 1.422882 </td>
   <td style="text-align:right;"> 17.179 </td>
   <td style="text-align:right;"> 48.40644 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> zonal_full </td>
   <td style="text-align:right;"> 48.26012 </td>
   <td style="text-align:right;"> 1.315792GB </td>
   <td style="text-align:right;"> 1.545439 </td>
   <td style="text-align:right;"> 2.742181 </td>
   <td style="text-align:right;"> 0.000 </td>
   <td style="text-align:right;"> 48.26012 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> intersectr_stage_weights </td>
   <td style="text-align:right;"> 83.96803 </td>
   <td style="text-align:right;"> 4.062578GB </td>
   <td style="text-align:right;"> 2.688918 </td>
   <td style="text-align:right;"> 8.466633 </td>
   <td style="text-align:right;"> 76.132 </td>
   <td style="text-align:right;"> 160.10003 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> exactextractr </td>
   <td style="text-align:right;"> 180.08758 </td>
   <td style="text-align:right;"> 491.349976MB </td>
   <td style="text-align:right;"> 5.766965 </td>
   <td style="text-align:right;"> 1.000000 </td>
   <td style="text-align:right;"> 0.000 </td>
   <td style="text-align:right;"> 180.08758 </td>
  </tr>
</tbody>
</table>

Overall when the polygon count is very high (~20,000), the zonal aproach with precomputed wieghts and non precomputed weights performs the best. Precomputing the weights save a significant amount of memory in the process and ~4 seconds in total run time. These timings suggest that 20 years of daily GridMet data could be computed for the ~20,000 catchments in about 11 minutes (~30 seconds * 20 year + ~15 seconds). The `intersectr` approach requires way more memory despite the precomputation of weights and takes about 3 times as long as zonal. Lastly the `exactextract` methods timed out at the upper limit of 180 seconds we prescribed. 

# Florida

Our second example looks at the timings for an aggregation over a large area with a few large aggregation units - Florida counties.

The gridded data and aggregate units we are working with can be seen below:


```r
florida <- AOI::aoi_get(state = "FL", county = "all") 
```

The same functions and timing from example 1 are computed:


```r
int_time_fl = system.time({
  intersectr_input_florida = intersectr_prep(file, florida, ID = "geoid", variable = 'potential_evapotranspiration')
})

zonal_time_fl = system.time({
  zonal_w_florida = weighting_grid(file, florida, ID = "geoid", progress = FALSE)
})
```



```r
fl_bnch <- bench::mark(
  iterations = 1, check = FALSE, time_unit = 's',
  intersectr_stage_weights = execute_intersection(nc_file = file,
                               variable_name = 'potential_evapotranspiration',
                               intersection_weights = intersectr_input_florida$w,
                               cell_geometry = intersectr_input_florida$grid, 
                               x_var = intersectr_input_florida$x,
                               y_var = intersectr_input_florida$y,
                               t_var = intersectr_input_florida$t, 
                               start_datetime = NULL, 
                               end_datetime = NULL),
  exactextractr            = exactrextract_process(file, florida, "geoid"),
  zonal_stage_weights      = execute_zonal(file, zonal_w_florida),
  zonal_full               = zonal_full(file, florida, "geoid")
)
```


<table class="table table-condensed">
 <thead>
  <tr>
   <th style="text-align:right;"> expression </th>
   <th style="text-align:right;"> medianTime </th>
   <th style="text-align:right;"> memoryAllocated </th>
   <th style="text-align:right;"> medianTime_rel </th>
   <th style="text-align:right;"> memoryAllocated_rel </th>
   <th style="text-align:right;"> preTime </th>
   <th style="text-align:right;"> TotalTime </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> zonal_stage_weights </td>
   <td style="text-align:right;"> 0.7843764 </td>
   <td style="text-align:right;"> 417.466MB </td>
   <td style="text-align:right;"> 1.000000 </td>
   <td style="text-align:right;"> 1.000000 </td>
   <td style="text-align:right;"> 0.161 </td>
   <td style="text-align:right;"> 0.9453764 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> zonal_full </td>
   <td style="text-align:right;"> 1.0140872 </td>
   <td style="text-align:right;"> 431.9606MB </td>
   <td style="text-align:right;"> 1.292858 </td>
   <td style="text-align:right;"> 1.034720 </td>
   <td style="text-align:right;"> 0.000 </td>
   <td style="text-align:right;"> 1.0140872 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> intersectr_stage_weights </td>
   <td style="text-align:right;"> 2.5669364 </td>
   <td style="text-align:right;"> 735.3934MB </td>
   <td style="text-align:right;"> 3.272582 </td>
   <td style="text-align:right;"> 1.761565 </td>
   <td style="text-align:right;"> 260.702 </td>
   <td style="text-align:right;"> 263.2689364 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> exactextractr </td>
   <td style="text-align:right;"> 83.6700304 </td>
   <td style="text-align:right;"> 475.1922MB </td>
   <td style="text-align:right;"> 106.670760 </td>
   <td style="text-align:right;"> 1.138278 </td>
   <td style="text-align:right;"> 0.000 </td>
   <td style="text-align:right;"> 83.6700304 </td>
  </tr>
</tbody>
</table>

Again, simular results are found.
